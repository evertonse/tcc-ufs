
\section{Análise Léxica (\texttt{lexer})} \label{section-lexer}


Nesta etapa, é realizado o processo de tokenização de um subconjunto dos símbolos possíveis no ambiente de equação do \LaTeX{}, conforme comentado na \autoref{especificacao-linguagem}. A entrada desse processo são caracteres do arquivo fonte, enquanto a saída é uma sequência lógica desses caracteres, organizada em \textit{tokens}. O código responsável por essa funcionalidade  está contido no pacote \texttt{lexer}.

O processo de análise léxica realiza uma varredura completa no arquivo de entrada, caractere por caractere, para identificar e extrair os \textit{tokens}. Antes de iniciar essa extração, verificamos se o trecho analisado pertence a um ambiente de equação. Essa verificação é feita ao identificar a \textit{string} \verb|\begin{equation}|, que marca o início da extração de \textit{tokens}. Da mesma forma, a delimitação do ambiente se encerra com a \textit{string} \verb|\end{equation}|. Isso permite que o sistema ignore partes do arquivo que não pertencem ao ambiente de equação, como textos explicativos ou outros elementos presentes no mesmo arquivo \texttt{.tex}. Dessa forma, garantimos que a tokenização seja restrita às secções relevantes do código.

    
% Para fins de documentação e maior clareza, definimos uma lista estruturada de expressões regulares que descreve a geração dos \textit{tokens}, apresentada na \autoref{grammar-tokens}\footnote{Vale observar que a "gramática dos \textit{tokens}" não possui atributos típicos de gramáticas livres de contexto, como recursão, símbolo inicial ou lados direitos compostos por não-terminais. Assim, pode ser mais adequado considerá-la como uma lista estruturada de expressões regulares.}.
%
% O alfabeto dessa gramática é composto pelos caracteres do arquivo fonte. Apesar de ser chamada de gramática, ela se assemelha mais a um conjunto de expressões regulares que descrevem padrões utilizados para identificar os \textit{tokens}. A escolha do termo "gramática" reflete a intenção de estabelecer uma a sintaxe de descrição da gramatica que será usada na gramática da análise sintática, a qual faz uso de uma gramática livre de contexto (GLC) para construção da árvore de sintaxe abstrata. Internamente, a geração dos \textit{tokens} é implementada como a simulação de uma máquina de estados finitos, que segue os padrões definidos por essas expressões regulares.

    % Para fins de documentação e maior clareza, definimos uma gramática que descreve a geração dos \textit{tokens}, apresentada na \autoref{grammar-tokens}\footnote{essa gramatica está mais para uma lista}. O alfabeto dessa gramática é composto pelos caracteres do arquivo fonte. Apesar de documentar com uma gramática, a geração dos \textit{tokens} é implementada internamente de maneira semelhante à simulação de uma máquina de estados.

Na lista de expressões regulares (\autoref{grammar-tokens}), definimos os tipos de \textit{tokens}, onde o lado esquerdo do símbolo ``$=$'' corresponde ao tipo de \textit{token}, e o lado direito descreve sua expressão regular. Palavras em letras maiúsculas representam categorias de caracteres, como \texttt{DIGIT}, que denota qualquer dígito de 0 a 9, e \texttt{LETTER}, que cobre letras de \texttt{`a'} a \texttt{`z'}. Já palavras entre aspas simples correspondem a sequências literais de caracteres.

Além disso, utilizamos os seguintes símbolos na notação: ``$*$'' indica zero ou mais ocorrências do caractere especificado; ``$|$'' representa alternativas para a geração do mesmo tipo de \textit{token}; e ``$;$'' marca o fim da definição do tipo de \textit{token}.

%%%%%%%%%%%%%

O pacote inteiro de tokenização pode ser acessado por meio de uma única função, descrita no \autoref{function-lex}, escrita na linguagem \texttt{Odin}. Essa função, chamada \texttt{lex}, aceita uma lista de caracteres como entrada e retorna uma lista de estruturas do tipo \texttt{Token} (detalhado no \autoref{lexer-structs}). A estrutura \texttt{Token} possui três campos principais:

\begin{itemize}
    \item \texttt{kind}: identifica o tipo de \textit{token}, mapeando-o para uma dos tipos definidos no \autoref{grammar-tokens}.
    \item \texttt{text}: contém a \textit{string} correspondente ao \textit{token} gerado.
    \item \texttt{position}: uma instância do tipo \texttt{Position}, que registra a posição exata do \textit{token} no arquivo de origem.
\end{itemize}


\begin{codigo}[H]
        \caption{\small Função principal do Lexer. }
        \label{function-lex}
\begin{lstlisting}[language = c]
  
    lex :: proc(input: []u8) -> []Token
\end{lstlisting}
\end{codigo}



Durante a iteração sobre o \texttt{input}, o processo de tokenização mantém algumas variáveis de controle para monitorar o estado do fluxo de caracteres. Quebras de linha são contadas ao encontrar sequências como \verb|"\n"| ou \verb|"\n\r"|. É mantida a coluna atual que rastreia a posição horizontal do caractere em uma linha. O cursor é o índice que aponta para o caractere atualmente em processamento. Essas informações são usadas para preencher o campo \texttt{position} de cada \textit{token}. A estrutura \texttt{Position}, detalhada no \autoref{lexer-structs}, é essencial para garantir a precisão na geração de relatórios e rastreamento de erros.

\subsection{Reporte de Erros} \label{subsection-erros}

O sistema informação de erros, implementado nesta etapa, é utilizado por todos os pacotes do projeto. Essa funcionalidade assegura que erros sejam associados a posições específicas no arquivo de entrada, facilitando a depuração e correção. A assinatura da função de tratamento de erros, bem como suas possíveis variações, está documentada no \autoref{cod-function-errors}.

\begin{codigo}[H]
    \caption{\small Função de erro exposto pelo pacote \texttt{lexer}. }
        \label{cod-function-errors}
\begin{lstlisting}[language=C++]
error_from_pos :: proc(pos: Position, msg: string, args: ..any)
error_from_token :: proc(token: Token, msg: string, args: ..any);
\end{lstlisting}
\end{codigo}


Dada uma posição ou um \texttt{token}, é exibida uma mensagem (\texttt{msg}) diretamente no terminal, formatada para destacar visualmente o erro em vermelho. A formatação utiliza as informações do \texttt{token}, como o nome do arquivo, a linha, a coluna e o comprimento do \texttt{token} problemático, permitindo sublinhar precisamente onde o erro ocorreu. Isso proporciona maior clareza às mensagens de erro, como exemplificado no caso de erro semântico devido ao uso de identificadores não definidos (\autoref{error-undefined-symbol}).

Optou-se por apresentar nesta seção uma visão geral de alguns erros possíveis para demonstrar como o compilador os reporta visualmente, independentemente de serem léxicos, sintáticos ou semânticos. Essa maneira evita sobrecarregar as seções de análise sintática e semântica com descrições ou imagens excessivas. Nas análises seguintes, os tipos de erro serão discutidos em suas etapas correspondentes. A seguir, são apresentados exemplos de erros possíveis:

\begin{enumerate}
   \item \textbf{Erros léxicos}: uso de palavras reservadas (\autoref{error-reserved-word}).
   
   \item \textbf{Erros sintáticos}: problemas de estrutura, como balanceamento incorreto de parênteses (\autoref{error-balanceamento}) e \textit{tokens} que não formam uma expressão matemática válida (\autoref{error-cant-make-expression}).
   
   \item \textbf{Erros semânticos}: envolvendo tipos incompatíveis (\autoref{error-incompatible-types}), símbolos não definidos (\autoref{error-undefined-symbol}) e redefinição de símbolos (\autoref{error-redefinition}).
\end{enumerate}


% Outros exemplos de erros seguem o mesmo padrão de exibição e incluem: tipos incompatíveis (\autoref{error-incompatible-types}), símbolos não definidos (\autoref{error-undefined-symbol}), balanceamento de parênteses (\autoref{error-balanceamento}), uso de palavras reservadas (\autoref{error-reserved-word}) .

\begin{figure}[H]
    \caption{\label{error-undefined-symbol} \small Erro ao tentar símbolo não definido.}
    \begin{center}
        \includegraphics[scale=0.5]{./Imagens/error-undefined-symbol.png}
    \end{center}
\end{figure}

\begin{figure}[H]
    \caption{\label{error-incompatible-types} \small Erro de tipos incompatíveis.}
    \begin{center}
        \includegraphics[scale=0.5]{./Imagens/error-incompatible-types.png}
    \end{center}
\end{figure}


\begin{figure}[H]
    \caption{\label{error-balanceamento} \small Erro de balanceamento de parênteses.}
    \begin{center}
        \includegraphics[scale=0.5]{./Imagens/error-balanceamento.png}
    \end{center}
\end{figure}

\begin{figure}[H]
    \caption{\label{error-reserved-word} \small Erro de uso incorreto de palavras reservadas.}
    \begin{center}
        \includegraphics[scale=0.5]{./Imagens/error-reserved-word.png}
    \end{center}
\end{figure}


\begin{figure}[H]
    \caption{\label{error-cant-make-expression} \small Erro de \textit{token} incapaz de produzir expressão.}
    \begin{center}
        \includegraphics[scale=0.5]{./Imagens/error-cant-make-expression.png}
    \end{center}
\end{figure}

\begin{figure}[H]
    \caption{\label{error-redefinition} \small Erro de redefinição de símbolo.}
    \begin{center}
        \includegraphics[scale=0.5]{./Imagens/error-redefinition.png}
    \end{center}
\end{figure}



\subsection{Classificação e Extração de Tokens}

Tokens simples, como aqueles compostos por um ou dois caracteres, são extraídos lendo-se o número correspondente de caracteres do \texttt{input}. Após a leitura, o \texttt{token} é construído e o laço continua para o próximo. Quando um caractere \texttt{\%} é encontrado, os caracteres subsequentes são ignorados até a próxima quebra de linha. Isso adiciona suporte a comentários no estilo \LaTeX{}.

Tokens mais complexos, como números, identificadores ou \textit{tokens} especiais, são extraídos com base em suas características. Números podem opcionalmente conter um ponto decimal, como em $1.0$. Já identificadores consistem em uma ou mais letras, opcionalmente prefixadas pelo símbolo \verb|\|.

A lista de expressões regulares de \textit{tokens} fornecida é intrinsecamente ambígua. Por exemplo, uma sequência como \verb|\frac| pode ser interpretada como um identificador comum ou como \verb"token_frac". Para resolver essa ambiguidade, criamos um dicionário que mapeia identificadores específicos para \textit{tokens} especiais (\autoref{map-special-identifiers}). Assim, se um identificador começar com o caractere \verb|\|, ele será verificado no dicionário e classificado como um \textit{token} especial, se apropriado.


\label{lexer-subexpression}
Identificadores não permitem números nem mesmo o caractere de sublinhado (\verb|_|). Isso ocorre porque, no analisador sintático, um nó do tipo identificador é modelado como um tipo recursivo, permitindo que identificadores sejam aninhados ao conter outros nós. Dessa forma, não é necessário permitir sublinhados diretamente no nível de token, o que possibilita a escrita de identificadores mais complexos, como \verb|\pi{n_1}| (renderizado em \LaTeX{} como $\pi_{n_1}$). Nesse caso, \verb|\pi| seria o primeiro \textit{token} do nó identificador, e sua subexpressão seria \verb|n_1| (renderizado como $n_1$), que, por sua vez, é o identificador \verb|n| com a subexpressão \verb|1|.


Adicionalmente, permitimos o uso da palavra-chave \verb|\text|, como em \verb|\text{id}|, para descrever identificadores. Essa palavra-chave é utilizada para incluir texto dentro do ambiente de equações em \LaTeX{}, que pode oferecer uma visualização mais clara de funções longas. Por exemplo, em vez de renderizar `\( normalize(x) \)', pode-se usar \verb|\text{normalize}(x)|, que será renderizado como `\( \text{normalize}(x) \)', tornando a expressão mais legível. Outro caso útil pode ser o \textit{token} `\( len \)', que pode ser visualmente confundido com a multiplicação entre \(l\), \(e\) e \(n\); usando \verb|\text{len}|, fica mais claro que `\(\text{len}\)' é um único \textit{token}, e não como uma multiplicação entre 3 outros \textit{tokens}. A extração do \textit{token} \verb|\text| segue um processo semelhante ao do suporte para \verb|\vec{id}|.

A enumeração que representa os tipos de \textit{tokens} pode ser consultada no \autoref{enum-token-kind}. Cada entrada dessa enumeração corresponde diretamente aos tipos de \textit{tokens} gerados pela lista de expressões regulares apresentada no \autoref{grammar-tokens}. Para facilitar a leitura, incluímos o símbolo correspondente à direita de cada entrada, indicado em comentários.

Com a etapa de tokenização concluída, é possível avançar para a análise sintática, onde é explorada a forma como os \textit{tokens} gerados são organizados em estruturas hierárquicas que formam a árvore sintática.

\begin{codigo}[H]
        \caption{\small Estruturas do Lexer. }
        \label{lexer-structs}
\begin{lstlisting}[language=C++]

Token :: struct {
    kind: Token_Kind,
    val: union{i64,f64},
    text: string,
    pos:  Position,
}

Position :: struct {
    file:   string,
    offset: i64,   // starting at 0, buffer offeset in file
    line:   i64,   // starting at 1, starting
    column: i64,   // starting at 1
    length: int    // how much chars foward
}
    
  \end{lstlisting}
\end{codigo}




\begin{codigo}[H]
    \caption{\small Gramática ilustrativa para \texttt{tokens}. }
        \label{grammar-tokens}
  \begin{lstlisting}[numbers=none, frame=none, language=haskell]

    token_number     = DIGIT DIGIT* '.' DIGIT DIGIT* | DIGIT DIGIT*;
    token_identifier = '\' LETTER LETTER* | LETTER LETTER*;
    token_cmpgreater = '>';
    token_cmpless    = '<';
    token_cmpequal   = '==';
    token_equal      = '=';
    token_mul        = '*' | '\cdot';
    token_cross      = '\times';
    token_div        = '/';
    token_plus       = '+';
    token_minus      = '-';
    token_caret      = '^';
    token_semicolon  = ';';
    token_comma      = ',';
    token_colon      = ':';
    token_question   = '?';
    token_bang       = '!';
    token_openparen  = '(';
    token_closeparen = ')';
    token_opencurly  = '{';
    token_closecurly = '}';
    token_tilde      = '~';
    token_underline  = '_'; --- Used for subexpresions
    token_arrow      = '->';
    token_begin      = '\begin';
    token_end        = '\end';
    token_frac       = '\frac';
    token_vec        = '\vec';
    token_omega      = '\omega';
    token_theta      = '\theta';
    token_phi        = '\phi';
    token_rho        = '\rho';
    token_alpha      = '\alpha';
    token_beta       = '\beta';
    token_sigma      = '\sigma';
    token_pi         = '\pi';
    token_epsilon    = '\epsilon';
    token_max        = '\max';
    token_min        = '\min';
    token_exp        = '\exp';
    token_tan        = '\tan';
    token_sin        = '\sin';
    token_cos        = '\cos';
    token_arctan     = '\arctan';
    token_arcsin     = '\arcsin';
    token_arccos     = '\arccos';
    token_sqrt       = '\sqrt';
    token_text       = '\text';
    token_eof        = EOF;
    
  \end{lstlisting}
\end{codigo}



\begin{codigo}[H]
\caption{\small Enumeração dos tipos de \texttt{tokens}. }
    \label{enum-token-kind}
\begin{lstlisting}[language = C++]
  Token_Kind :: enum {
    EOF           = 0,
    Number,
    Identifier,

    Equal,        // =
    Mul,          // * ou \cdot
    Cross,        // X
    Div,          // /
    Plus,         // +
    Minus,        // -
    Caret,        // ^
    Comma,        // ,
    Colon,        // :
    Question,     // ?
    Bang,         // !
    OpenParen,    // (
    CloseParen,   // )
    OpenCurly,    // {
    CloseCurly,   // }
    Tilde,        // ~
    Underline,    // _
    Arrow,        // ->

    Begin = 256,  // \begin
    End,          // \end

    Frac,         // \frac
    Vec,          // \vec

    Omega,        // \omega
    Theta,        // \theta
    Phi,          // \phi
    Rho,          // \rho
    Pi,           // \pi
    Epsilon,      // \epsilon
    Alpha,        // \alpha
    Beta,         // \beta
    Sigma,        // \sigma

    Max,          // \max
    Min,          // \min
    Exp,          // \exp
    Tan,          // \tan
    ArcTan,       // \arctan
    Sin,          // \sin
    ArcSin,       // \arcsin
    Cos,          // \cos
    ArcCos,       // \arccos
    Sqrt,         // \sqrt

    Text,         // \text
    Invalid
}

\end{lstlisting}
\end{codigo}

\begin{codigo}[H]
        \caption{\small Mapa de identificadores especiais. }
        \label{map-special-identifiers}
  \begin{lstlisting}[language=C++]

SPECIAL_WORDS := map[string]Token{
    "text"  = Token{text = "\\text",     kind =.Text},

    // Special
    "frac"   = Token{text = "\\frac",    kind =.Frac},
    "vec"    = Token{text = "\\vec",     kind =.Vec},
    "cdot"   = Token{text = "\\cdot",    kind =.Mul},
    "begin"  = Token{text = "\\begin",   kind =.Begin},
    "end"    = Token{text = "\\end",     kind =.End},
    "rho"    = Token{text = "\\rho",     kind =.Rho},
    "sqrt"   = Token{text = "\\sqrt",    kind =.Sqrt},
    "omega"  = Token{text = "\\omega",   kind =.Omega},

    // Cross product
    "times"  = Token{text = "\\times",   kind =.Cross},

    "max"    = Token{text = "\\max",     kind =.Max},
    "min"    = Token{text = "\\min",     kind =.Min},
    "exp"    = Token{text = "\\exp",     kind =.Exp},

    "cos"    = Token{text = "\\cos",     kind =.Cos},
    "sin"    = Token{text = "\\sin",     kind =.Sin},
    "tan"    = Token{text = "\\tan",     kind =.Tan},

    "arccos" = Token{text = "\\arccos",  kind =.ArcCos},
    "arcsin" = Token{text = "\\arcsin",  kind =.ArcSin},
    "arctan" = Token{text = "\\arctan",  kind =.ArcTan},

    "theta"  = Token{text = "\\theta",   kind =.Theta},
    "phi"    = Token{text = "\\phi",     kind =.Phi},

    "alpha"   = Token{text = "\\alpha",  kind =.Alpha},
    "beta"    = Token{text = "\\beta",   kind =.Beta},
    "sigma"   = Token{text = "\\sigma",  kind =.Sigma},
    "pi"      = Token{text = "\\pi",     kind =.Pi},
    "epsilon" = Token{text = "\\epsilon",kind =.Epsilon},
}

    
  \end{lstlisting}
\end{codigo}
