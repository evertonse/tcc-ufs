
\section{Análise Sintática, (\texttt{parser})} \label{section-parser}

O \textit{parser} para a linguagem subconjunto do ambiente \verb|equation| do \LaTeX{} foi desenvolvido utilizando o método de Pratt \textit{Parsing} na linguagem Odin. Neste contexto, esse subconjunto é denominado como \texttt{EquationLang}, que abrange todas as partes essenciais para a definição de BRDFs descritas na \autoref{especificacao-linguagem}, e sua gramática é documentada.

A implementação deste \textit{parser} adota o método de descida recursiva, no qual cada regra de produção da gramática possui uma função de análise correspondente. Este método foi escolhido por priorizar a  simplicidade do código e clareza da gramática. A entrada do \textit{parser} consiste nos \textit{tokens} gerados na etapa de análise léxica.


\subsection{Sintaxe de Gramáticas} \label{section-sintatic-grammar-notation}
Na definição de gramáticas deste capítulo, utilizamos uma notação simplificada de sintaxe para representar as regras. Palavras compostas por letras minúsculas representam \textit{não-terminais}, enquanto palavras entre aspas simples correspondem a \textit{terminais}.

Além disso, os seguintes operadores são utilizados na notação:
\begin{itemize}
    \item ``$*$'' indica zero ou mais ocorrências de um elemento, que pode ser tanto um \textit{terminal} quanto um \textit{não-terminal};
    \item ``$()$'' define agrupamentos para aplicar um operador sobre os elementos agrupados;
    \item ``$|$''  introduz uma regra alternativa para o mesmo \textit{não-terminal}. Caso esteja dentro de um agrupamento, como em ``$(a|b)$'', significa que aceita $a$ ou $b$;
    \item ``$=$''  representa uma produção.
\end{itemize}

Adicionalmente, sequências de três hífens (``\verb|---|'') são usadas para comentários destinados ao leitor, sem impacto na definição gramatical.

No caso da gramática da \texttt{EquationLang}, apresentada na \autoref{subsection-grammar}, todas as palavras em letras minúsculas que começam com \texttt{token\_} são tratadas como \textit{terminais}. Esses terminais correspondem diretamente aos \textit{tokens} gerados pelo \texttt{lexer} e estão presentes na lista de expressões regulares descrita na \autoref{section-lexer}.

% %%%
% Na definição de gramáticas deste capítulo, é utilizado uma notação leve de sintaxe para representar suas regras. Palavras com todas as letras minúsculas representam não-terminais, enquanto palavras entre aspas simples correspondem a uma sequência literal de terminais.
%
% Ainda, definimos símbolos operadores: ``$*$'' indica zero ou mais ocorrências; ``$()$'' indica agrupamento para aplicar um operador@@agrupa oq@@@; ``$|$'' simboliza o início de uma regra alternativa para o mesmo não-terminal, ou se estiver dentro de um agrupamento, como por exemplo``$(a|b)$'', significa que aceita $a$ ou $b$; e ``$=$'' indica uma produção.
%
% com o adicional que sequências de três hífens ("\verb"---"") representam comentários para o leitor, sem impactar a definição gramatical.
%
% No caso da gramática da \texttt{EquationLang}, todas as palavras com letras minusculos que começam com \texttt{token\_}, são terminais. Esses terminals estão definilos


% Essa mesma sintaxe de gramática é utilizada na análise sintática, onde cada regra faz bijeção com os tipos de nó da AST, como detalhado na \autoref{section-parser}, porém o alfabeto passa a ser composto pelo conjunto de \textit{tokens} gerados pelo \texttt{lexer}.


\subsection{Parser} \label{section-parser-pratt}
Diferentemente dos \textit{parsers} tradicionais de descida recursiva, que frequentemente utilizam múltiplas chamadas de função indiretamente recursivas para tratar diferentes níveis de precedência, o \textit{parser} aqui implementado organiza as funções de análise com base na tabela de precedência dos operadores. Essa abordagem é detalhada no \autoref{alg-pratt-parsing}, que descreve a lógica central para o \textit{parsing} de expressões.

O Pratt Parsing simplifica a análise sintática de expressões ao tratar precedência e associatividade de forma dinâmica, sem inflar a gramática. Na abordagem tradicional, as precedências são fixadas por meio de várias regras de produção, como demonstrado neste exemplo de expressões binárias no \autoref{cod-regras-tradicionais}. Regras intermediárias são adicionadas à gramática com o principal propósito de embutir a precedência na estrutura gramatical.

\begin{codigo}[htb]
    \caption{\small Regras tradicionais de precedência por gramática. }
    \label{cod-regras-tradicionais}
\begin{lstlisting}[language=haskell, numbers=none, inputencoding=utf8]
    expr    = or_expr;
    or_expr = or_expr '||' and_expr | and_expr;

    and_expr = and_expr '&&' eq_expr| eq_expr;

    ---  Igualdade
    eq_expr = eq_expr '==' rel_expr | rel_expr;   

    ---  Comparação
    rel_expr = rel_expr '<' add_expr | add_expr;     

    add_expr = add_expr '+' mul_expr | mul_expr;

    mul_expr = mul_expr '*' primary_expr | primary_expr;

    primary_expr = '(' expr ')' | NUMBER | IDENTIFIER; 
\end{lstlisting}
\end{codigo}

Isso torna a gramática mais longa, exigindo mais regras que derivam outras antes de chegar a um terminal. Na descida recursiva, as regras atuam como funções, o que significa que métodos recursivos tradicionais com muitos níveis de precedência resultam em um maior número de chamadas recursivas, aumentando a complexidade da implementação e a rigidez na modificação da gramática. Em contraste, no Pratt Parsing, uma única regra é suficiente, juntamente com uma tabela de precedência: \verb"expr = expr ( '||' | '&&' | '==' | '<' | '+' | '*' ) expr";.

O \textit{parser} desenvolvido para \texttt{EquantionLang} consulta à tabela de precedência dinamicamente, conforme definida na \autoref{tab-token-precedence}, para decidir a ordem de avaliação com base no \textit{token} operador encontrado.


Foi utilizada uma notação semelhante à original de Pratt \cite{pratt}, onde as funções \verb"parse_null_denotations" e \verb"parse_left_denotations" desempenham papéis equivalentes às funções \texttt{token.prefixo} e \texttt{token.infixo}, respectivamente, como demonstrado no \autoref{alg1}. 

Além disso, a abordagem de descida recursiva (top-down) permite que cada regra de produção definida na gramática (\autoref{grammar-ast-pt1}) seja diretamente mapeada para um procedimento em código. Isso pode ser observado, por exemplo, na função de análise do nó \texttt{Start} da AST (\autoref{cod-parsing-start}), que reflete diretamente as regras de produção \texttt{start}, \texttt{decl} e \verb"decl_equation_begin_end_block" especificadas na gramática apresentada no \autoref{grammar-ast-pt1}.

Do ponto de vista da interface oferecida pelo pacote \texttt{parser}, todo o processo de análise sintática é abstraído em uma única chamada de função (\autoref{cod-func-and-structs}). A função principal, \texttt{parse}, trabalha em conjunto com a estrutura \texttt{Parser} para realizar a análise.

\begin{codigo}[H]
  \caption{\small Parsing de expressão em código Odin.}
        \label{alg-pratt-parsing}
  \begin{lstlisting}[language=C]


parse_expr :: proc(prec_prev: i64) -> ^Expr {
    /* expressions that takes nothing (null) as left operand */
    left := parse_null_denotations()
    /*
    . if current token is left associative or current token has higher precedence
    . than previous precedence then stay in the loop, effectively creating a left leaning
    . sub-tree, else, we recurse to create a right leaning sub-tree.
    */
    for precedence(peek()) > prec_prev + associativity(peek())  {
        /* expressions that needs a left operand such as postfix, mixfix, and infix operator */
        left = parse_left_denotations(left)
    }
    return left
}


  \end{lstlisting}
\end{codigo}

\begin{codigo}[htb]
    \caption{\small Estrutura e função principal do pacote \texttt{parser}. }
        \label{cod-func-and-structs}
  \begin{lstlisting}[language = C]
Parser :: struct {
    tokens:      []Token,
    cursor:      i64,
    error_count: int,
}

parse :: proc(using p: ^Parser) -> ^ast.Start {
    return parse_start(p)
}
  \end{lstlisting}
\end{codigo}

\begin{codigo}[htb]
    \caption{\small \textit{Parsing} do nó \texttt{Start}. }
        \label{cod-parsing-start}
  \begin{lstlisting}[language=C++]
parse_start :: proc(using p: ^Parser) -> ^ast.Start {
    node := ast.new(ast.Start)
    decls := [dynamic]^ast.Decl{}

    for peek(p).kind != .EOF {
        decl := parse_equation_begin_end_block(p)
        append(&decls, decl)
    }
    node.eof = next(p, Token_Kind.EOF)
    node.decls = decls[:]
    return node
}
    
  \end{lstlisting}
\end{codigo}



\subsection{Gramática} \label{subsection-grammar}


Para formalizar a gramática da linguagem de entrada (\texttt{EquationLang}), suas regras estão detalhadas nos \autoref{grammar-ast-pt1} e \autoref{grammar-ast-pt2}. Um exemplo de código-fonte contendo três equações válidas nessa linguagem se encontra no \autoref{code-gramatica}. Sua renderização correspondente em \LaTeX{} é ilustrada na \autoref{code-gramatica-rendered}.

Nesse exemplo, a primeira equação ($\rho_{d}$) é gerado pela regra \verb"expr_vector_literal". A segunda ($\rho_{s}$), é uma expressão binária derivada da regra \verb"expr_infix". Já a última equação utiliza múltiplas regras, como \verb"expr_grouped" e \verb"expr_prefix".


\begin{subequations}
\label{code-gramatica-rendered} 
\begin{equation}
    \rho_{d} = \vec{0.3,0.3,0.3}
\end{equation}
\begin{equation}
    \rho_{s} = \vec{0.0,0.2,1.0} * 20
\end{equation}
\begin{equation}
f = \frac{\rho_{d}}{\pi} + \frac{\rho_{s}}{8*\pi} * \frac{({\vec{n}}\cdot{\vec{h}})} {({\vec{\omega_{o}}}\cdot{\vec{h}}) * \max(({\vec{n}}\cdot{\vec{\omega_{i}}}), ({\vec{n}}\cdot{\vec{\omega_{o}}}))}
\end{equation}
\end{subequations}


\begin{codigo}[htb]
        \caption{\small Exemplo código escrito na linguagem \texttt{EquationLang}. }
        \label{code-gramatica}
\begin{lstlisting}[language=tex, frame=none]
\begin{equation}
    \rho_{d} = \vec{0.3,0.3,0.3}
\end{equation}

\begin{equation}
    \rho_{s} = \vec{0.0,0.2,1.0}*20
\end{equation}

\begin{equation}
f = \frac{\rho_{d}}{\pi} + \frac{\rho_{s}}{8*\pi} *
\frac{({\vec{n}}\cdot{\vec{h}})} {({\vec{\omega_{o}}}\cdot{\vec{h}}) *
\max(({\vec{n}}\cdot{\vec{\omega_{i}}}), ({\vec{n}}\cdot{\vec{\omega_{o}}}))}
\end{equation}

\end{lstlisting}
\end{codigo}

% \tiny - Very small text
% \scriptsize - Smaller than small
% \footnotesize - Slightly smaller than small
% \small - Small text
% \normalsize - Normal text (default size)
% \large - Larger than normal
% \Large - Even larger
% \LARGE - Larger still
% \huge - Very large
% \Huge - Largest size
\begin{codigo}[H]
        \caption{\small Gramática para \texttt{EquantionLang} parte 1.}
        \label{grammar-ast-pt1}
        
% \begin{lstlisting}[language=haskell, basicstyle=\footnotesize, numbers=none, inputencoding=utf8]
\begin{lstlisting}[language=haskell, basicstyle=\ttfamily\footnotesize, numbers=none, inputencoding=utf8]
    start  = decl* token_eof;
    decl   = decl_equation_begin_end_block;
    decl_equation_begin_end_block =
        token_begin token_opencurly 'equation' token_closecurly
        decl_equation
        token_end token_opencurly 'equation' token_closecurly;
    decl_equation = field;
    field = expr token_equal expr;

    expr = expr_identifier
        | expr_number  | expr_vector_literal
        | expr_grouped | expr_prefix | expr_infix
        --- Ressalto que `function_call` e `function_definition` tem a mesma construção.
        --- Apenas diferenciamos pela posição que aparecer, se à esquerda ou à direta de '=' da regra `field`.
        --- por exemplo `a = f(1)`, `f(1)` é uma chamada de função
        --- Já `f(x) = 1` é uma definição de função
        | expr_function_call | expr_function_definition
        | token_opencurly expr token_closecurly
    ;
    expr_identifier =
        --- Ex: `\text{id}`
        token_text token_opencurly expr_identifier token_closecurly
        --- Ex: `\vec{id}`
        token_vec token_opencurly expr_identifier token_closecurly
        --- Ex: `id_n`
        | expr_identifier token_underline expr_identifier
        --- Ex: `id_2`
        | expr_identifier token_underline token_number
        --- Ex: `id_{n+1}`
        | expr_identifier token_underline token_opencurly expr token_closecurly
        --- Token especiais como \phi ou \alpha
        | token_identifier | token_omega   | token_theta | token_phi
        | token_rho        | token_alpha   | token_beta  | token_sigma
        | token_pi         | token_epsilon | token_max   | token_min
    ;

\end{lstlisting}
\end{codigo}

\begin{codigo}[H]
        \caption{\small Gramática para \texttt{EquantionLang} parte 2.}
        \label{grammar-ast-pt2}
    % basicstyle=\fontsize{6}{7.2}\selectfont\ttfamily
\begin{lstlisting}[language=haskell, basicstyle=\ttfamily\footnotesize,numbers=none, inputencoding=utf8]
    expr_number = token_number;

    expr_vector_literal = token_vec
        --- Ex: `\vec{1, 1, 1}`
        token_opencurly
        (expr_number token_comma)* expr_number
        token_closecurly
    ;

    expr_grouped = token_openparen expr token_closeparen;

    expr_prefix =
        (token_sqrt | token_exp | token_tan| token_cos | token_sin | token_arctan | token_arccos | token_arcsin | token_minus | token_plus) expr
    ;

    expr_infix = token_frac
        token_opencurly expr token_closecurly
        token_opencurly expr token_closecurly
        | expr token_plus     expr
        | expr token_minus    expr
        | expr token_mul      expr
        | expr token_cross    expr
        | expr token_cmpequal expr
        | expr token_div      expr
        | expr token_caret    expr
    ;

    expr_function_call = expr token_openparen
        (expr token_comma)* expr
        token_closeparen
    ;

    --- Mesmo que expr_function_call, em etapas posteriores é decidido qual tipo realmente é.
    expr_function_definition = expr token_openparen
        (expr token_comma)* expr
        token_closeparen
    ;
\end{lstlisting}
\end{codigo}

Na definição da gramática (\autoref{grammar-ast-pt1}), adotamos a notação sintática previamente estabelecida na \autoref{section-sintatic-grammar-notation}. 
A gramática apresentada inclui regras para expressões, atribuições, agrupamento, literais numéricos e vetoriais, chamadas e definições de funções, além de operadores, como \verb"expr_prefix" e \verb"expr_infix". Ela é projetada para suportar a sintaxe necessária à definição de BRDFs em \LaTeX{}.



A tabela de operadores (\autoref{tab-token-precedence}), usada no Pratt Parsing, é implementada pela função \texttt{precedence\_from\_token}, que associa cada \textit{token} a um valor inteiro indicando sua precedência: valores maiores indicam maior prioridade. Alguns \textit{tokens} assumem diferentes funções conforme o contexto e podem alterar sua precedência. Por exemplo, \texttt{(} pode ser um prefixo em expressões agrupadas, \textbf{(}$2*3$\textbf{)}, ou um infixo em chamadas de função, $f$\textbf{(}$x$\textbf{)}. De forma análoga, \texttt{-} pode atuar como operador de negação (prefixo) ou como operador de subtração (infixo). 


\begin{table}[h!]
\centering
\begin{tabular}{|l|c|c|}
\hline
\textbf{Tipo de Token} & \textbf{Prefixo} & \textbf{Precedência}\\ \hline
\texttt{+}            & Sim              & 25                   \\ \hline
\texttt{-}            & Sim              & 25                   \\ \hline
\texttt{(}            & Sim              & 100                  \\ \hline
\texttt{:}            & Sim              & 100                  \\ \hline
\texttt{*}            & Sim              & 100                  \\ \hline
% \texttt{\textasciitilde} & Sim              & 200               \\ \hline
\texttt{!}            & Sim              & 300                  \\ \hline
\texttt{(}            & Não              & 500                  \\ \hline
\texttt{>}            & Não              & 5                    \\ \hline
\texttt{<}            & Não              & 5                    \\ \hline
\texttt{+}            & Não              & 10                   \\ \hline
\texttt{-}            & Não              & 10                   \\ \hline
$\times$              & Não              & 20                   \\ \hline
\texttt{*}            & Não              & 20                   \\ \hline
\texttt{/}            & Não              & 20                   \\ \hline
\texttt{\textasciicircum} & Não           & 30                  \\ \hline
\texttt{!}            & Não              & 400                  \\ \hline
\end{tabular}
\caption{Tabela de Precedência dos Tokens}
\label{tab-token-precedence}
\end{table}


\subsubsection{Estrutura da Árvore de Sintaxe}

Esta seção apresenta os tipos de nós que compõem a AST do compilador da linguagem \texttt{EquationLang}. Cada estrutura de nó captura diferentes regras de produção da sintaxe em nível de código. O nó \texttt{Expr}, o mais genérico, inclui o campo \verb"ty_inferred", que armazena o tipo inferido da expressão, preenchido durante a análise semântica e utilizado na geração de código.

\begin{itemize}
\item \textbf{Node}: Estrutura base para todos os nós da AST. Representa um tipo genérico e é estendido por outros tipos mais específicos.

\item \textbf{Expr}: Representa expressões de forma geral, como literais, operadores, ou agrupamentos.

\item \textbf{Decl}: Representa declarações genéricas na linguagem, como definições de funções ou equações.

\item \textbf{Start}: O nó raiz da AST, contendo todas as declarações e marcando o fim do arquivo.

\item \textbf{Decl\_Equation}: Representa uma equação, conectando uma expressão ou valor a um identificador.

\item \textbf{Field}: Representa uma atribuição, associando um nome a um valor através do operador \texttt{=}.

\item \textbf{Expr\_Identifier}: Representa identificadores, como variáveis ou constantes, podendo incluir subexpressões e anotações adicionais.

\item \textbf{Expr\_Number}: Representa literais numéricos, como inteiros ou números de ponto flutuante.

\item \textbf{Expr\_Vector\_Literal}: Representa vetores literais, contendo uma sequência de valores numéricos.

\item \textbf{Expr\_Grouped}: Representa expressões agrupadas por delimitadores, como parênteses ou chaves.

\item \textbf{Expr\_Prefix}: Representa expressões prefixas, onde o operador precede o valor, como \texttt{-3}.

\item \textbf{Expr\_Infix}: Representa expressões infixas, onde o operador está entre dois valores, como \texttt{3 + 3}.

\item \textbf{Expr\_Function\_Call}: Representa chamadas de função, contendo o nome da função e seus argumentos.

\item \textbf{Expr\_Function\_Definition}: Representa definições de funções, incluindo o nome, parâmetros e o corpo da função.
\end{itemize}

Anteriormente, na \autoref{lexer-subexpression}, foi discutido que o \textit{parser} suporta identificadores aninhados, como \( x_{i_1} \) (\verb"x_{i_1}"). Esse suporte é demonstrado no \autoref{cod-expression-ident-recursive}, que processa identificadores de forma recursiva. O trecho faz parte de uma função maior onde um \texttt{switch}\footnote{Os comandos \texttt{switch} e \texttt{case} na linguagem Odin são semelhantes aos da linguagem C.} é aplicado sobre a enumeração descrita na \autoref{enum-token-kind}. Dentro desse \texttt{switch}, há um \texttt{case} específico que reconhece \textit{tokens} de identificadores ou símbolos especiais, como \( \omega, \theta, \phi, \rho, \alpha, \beta, \sigma, \pi, \epsilon \).

A recursividade, realizada por meio da função \texttt{parse\_expr}, permite incluir subíndices numéricos, subexpressões de identificadores ou expressões mais complexas, como \( n+1 \) em \( f_{n+1} \). Isso amplia a flexibilidade na representação de equações de BRDFs, onde o uso de subíndices numéricos é bastante comum.

\begin{codigo}[htb]
    \caption{\small Parte do código de \textit{parsing} de expressão para identificadores. }
        \label{cod-expression-ident-recursive}
  \begin{lstlisting}[language = C]
    case .Identifier,
         .Omega,     // \omega
         .Theta,     // \theta
         .Phi,       // \phi
         .Rho,       // \rho
         .Alpha,     // \alpha
         .Beta,      // \beta
         .Sigma,     // \sigma
         .Pi,        // \pi
         .Epsilon,   // \epsilon
        node := ast.new(ast.Expr_Identifier)
        node.identifier = next(p)
        if peek(p).kind == Token_Kind('_') {
            next(p)
            if peek(p).kind == Token_Kind('{') {
                next(p, '{')
                node.sub_expression = parse_expr(p, prec)
                next(p, '}')
            } else {
                //
                // If we're not using `identifier_{ }` then, we only allow simple number or identifier
                //
                sub_node := ast.new(ast.Expr_Identifier)
                if peek(p).kind == .Number {
                    // We only allow number as sub expressions
                    sub_node.identifier = next_expects_kind(p, .Number)
                } else {
                    sub_node.identifier = next_expects_kind(p, .Identifier, ..SPECIAL_IDENTIFIERS[1:])
                }
                node.sub_expression = sub_node
            }
        }
    
  \end{lstlisting}
\end{codigo}

Essa mesma lógica exemplifica o processamento de todas as expressões recursivas, como operações binárias. Para identificar o \textit{token} atual, utilizamos a função \texttt{peek()}, que permite visualizar um ou dois \textit{tokens} à frente e decidir qual nó da AST será construído. Após identificar o \textit{token}, calcula-se a variável \texttt{prec}, que determina a precedência do operador. Com base nessa precedência, são realizadas chamadas recursivas à função \texttt{parse\_expr} para processar expressões aninhadas.

Uma vez preenchidos todos os campos necessários, a expressão completa é retornada. Esse processo se repete até que toda a subárvore de expressões de uma equação seja construída. A análise sintática é concluída quando todas as equações são adicionadas à AST.

Posteriormente, essa estrutura hierárquica é anotada com tipos e validada pelo pacote \texttt{checker}, conforme descrito na \autoref{section-checker}. Antes da validação, é necessário implementar métodos para a travessia da árvore. Esse processo é detalhado na \autoref{section-walker}, que apresenta técnicas para percorrer a árvore, acessar seus dados e facilitar as etapas subsequentes de análise semântica e geração de código.

